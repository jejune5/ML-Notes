{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Seq2Seq循环神经网络编码器\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): vocab大小。\n",
    "        embedding_dim (int): 嵌入层输出维度。\n",
    "        hidden_size (int): RNN输出维度。\n",
    "        num_layers (int): RNN层数。\n",
    "        dropout (float): dropout。\n",
    "\n",
    "    Inputs:\n",
    "        x: (batch_size, seq_len)\n",
    "\n",
    "    Outputs:\n",
    "        output: (batch_size, seq_len, hidden_size)\n",
    "        state: (num_layers, batch_size, hidden_size)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout=0, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        super(Seq2SeqEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True,\n",
    "                          dropout=dropout)\n",
    "\n",
    "    def forward(self, x, *args):\n",
    "        # x->(batch_size, seq_len)\n",
    "        x = self.embedding(x)\n",
    "        # x->(batch_size, seq_len, embedding_dim)\n",
    "        output, state = self.rnn(x)\n",
    "        # output->(batch_size, seq_len, hidden_size)\n",
    "        # state->(num_layers, batch_size, hidden_size)\n",
    "        return output, state\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Seq2Seq循环神经网络解码器\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): vocab大小。\n",
    "        embedding_dim (int): 嵌入层输出维度。\n",
    "        hidden_size (int): RNN输出维度。\n",
    "        num_layers (int): RNN层数。\n",
    "        dropout (float): dropout。\n",
    "\n",
    "    Inputs:\n",
    "        X: (batch_size, seq_len)\n",
    "        state: (num_layers, batch_size, hidden_size)\n",
    "\n",
    "    Outputs:\n",
    "        output: (batch_size, num_steps, vocab_size)\n",
    "        state: (num_layers, batch_size, hidden_size)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim + hidden_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                          dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        enc_output, enc_state = enc_outputs\n",
    "        return enc_state\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        # x->(batch_size, seq_len)\n",
    "        x = self.embedding(x)\n",
    "        # X->(batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # state[-1]->(batch_size, hidden_size)\n",
    "        context = state[-1].repeat(x.shape[1], 1, 1)\n",
    "        # context->(seq_len, batch_size, hidden_size)\n",
    "        context = context.permute(1, 0, 2)\n",
    "        # context->(batch_size, seq_len, hidden_size)\n",
    "        # 拼接x与编码器最后隐状态\n",
    "        x_and_context = torch.cat((x, context), 2)\n",
    "        # X_and_context (batch_size, seq_len, embedding_dim + hidden_size)\n",
    "        output, state = self.rnn(x_and_context, state)\n",
    "        # output: (batch_size, seq_len, hidden_size)\n",
    "        # state:　(num_layers, batch_size, hidden_size)\n",
    "        output = self.fc(output)\n",
    "        # output: (batch_size, seq_len, vocab_size)\n",
    "        return output, state\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    合并Encoder与Decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_x, dec_x, *args):\n",
    "        enc_outputs = self.encoder(enc_x, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_x, dec_state)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 15, 3000])\n",
      "torch.Size([3, 32, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "seq_len = 15\n",
    "vocab_size = 3000\n",
    "embedding_dim = 100\n",
    "hidden_size = 8\n",
    "num_layers = 3\n",
    "\n",
    "enc_x = torch.zeros(batch_size, seq_len, dtype=torch.long)\n",
    "dec_x = torch.ones(batch_size, seq_len, dtype=torch.long)\n",
    "\n",
    "encoder = Seq2SeqEncoder(vocab_size, embedding_dim, hidden_size, num_layers)\n",
    "decoder = Seq2SeqDecoder(vocab_size, embedding_dim, hidden_size, num_layers)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "\n",
    "output, state = model(enc_x, dec_x)\n",
    "print(output.shape)\n",
    "print(state.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\n\\nfrom Example.CMN_ENG.config import *\\n\\nencoder = Seq2SeqEncoder(\\n    vocab_size=999,\\n    embed_size=EMBED_SIZE,\\n    num_hiddens=NUM_HIDDENS,\\n    num_layers=NUM_LAYERS,\\n    dropout=DROPOUT\\n)\\ndecoder = Seq2SeqDecoder(\\n    vocab_size=3333,\\n    embed_size=EMBED_SIZE,\\n    num_hiddens=NUM_HIDDENS,\\n    num_layers=NUM_LAYERS,\\n    dropout=DROPOUT\\n)\\nnet = EncoderDecoder(encoder=encoder, decoder=decoder)  # .to(DEVICE)\\nenc_X = torch.randint(100, (BATCH_SIZE, MAX_LENGTH))\\nenc_valid_len = 10\\n\\ny, _ = net(enc_X, enc_X)\\nprint('net:', y.shape)\\nenc_outputs = net.encoder(enc_X, 10)\\n# print('enc_outputs: ', enc_outputs.shape)\\n\\ndec_state = net.decoder.init_state(enc_outputs, 10)\\nprint('dec_state: ', dec_state.shape)\\n\\ndec_output, _ = net.decoder(enc_X, dec_state)\\nprint('dec_output: ', dec_output.shape)\\nprint(enc_X.shape)\\nprint(dec_output.shape)\\n\""
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "from Example.CMN_ENG.config import *\n",
    "\n",
    "encoder = Seq2SeqEncoder(\n",
    "    vocab_size=999,\n",
    "    embed_size=EMBED_SIZE,\n",
    "    num_hiddens=NUM_HIDDENS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "decoder = Seq2SeqDecoder(\n",
    "    vocab_size=3333,\n",
    "    embed_size=EMBED_SIZE,\n",
    "    num_hiddens=NUM_HIDDENS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "net = EncoderDecoder(encoder=encoder, decoder=decoder)  # .to(DEVICE)\n",
    "enc_X = torch.randint(100, (BATCH_SIZE, MAX_LENGTH))\n",
    "enc_valid_len = 10\n",
    "\n",
    "y, _ = net(enc_X, enc_X)\n",
    "print('net:', y.shape)\n",
    "enc_outputs = net.encoder(enc_X, 10)\n",
    "# print('enc_outputs: ', enc_outputs.shape)\n",
    "\n",
    "dec_state = net.decoder.init_state(enc_outputs, 10)\n",
    "print('dec_state: ', dec_state.shape)\n",
    "\n",
    "dec_output, _ = net.decoder(enc_X, dec_state)\n",
    "print('dec_output: ', dec_output.shape)\n",
    "print(enc_X.shape)\n",
    "print(dec_output.shape)\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}