{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. tokenizer 加载预训练字典和分词方法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(PreTrainedTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}),\n ['选择珠江花园的原因就是方便。',\n  '笔记本的键盘确实爽。',\n  '房间太小。其他的都一般。',\n  '今天才知道这书还有第6卷,真有点郁闷.',\n  '机器背面似乎被撕了张什么标签，残胶还在。'])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',\n",
    "    cache_dir=None,\n",
    "    force_download=False,\n",
    ")\n",
    "\n",
    "sents = [\n",
    "    '选择珠江花园的原因就是方便。',\n",
    "    '笔记本的键盘确实爽。',\n",
    "    '房间太小。其他的都一般。',\n",
    "    '今天才知道这书还有第6卷,真有点郁闷.',\n",
    "    '机器背面似乎被撕了张什么标签，残胶还在。',\n",
    "]\n",
    "\n",
    "tokenizer, sents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 普通句子编码"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]\n",
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": "'[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tokenizer.encode(\n",
    "    text=sents[0],\n",
    "    text_pair=sents[1],\n",
    "    # 大于maxlen，是否截断\n",
    "    truncation=True,\n",
    "\n",
    "    # 补齐pad到max_len\n",
    "    padding='max_length',\n",
    "    add_special_tokens=True,\n",
    "    max_length=30,\n",
    "    # pt, tf, np, ajax, 默认list\n",
    "    return_tensors=None\n",
    ")\n",
    "\n",
    "print(out)\n",
    "print(len(out))\n",
    "tokenizer.decode(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 增强型编码"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0], [101, 2791, 7313, 1922, 2207, 511, 1071, 800, 4638, 6963, 671, 5663, 511, 102, 791, 1921, 2798, 4761, 6887, 6821, 741, 6820, 3300, 5018, 127, 1318, 117, 4696, 3300, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'special_tokens_mask': [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], 'length': [27, 30], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": "'[CLS] 房 间 太 小 。 其 他 的 都 一 般 。 [SEP] 今 天 才 知 道 这 书 还 有 第 6 卷, 真 有 [SEP]'"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=[(sents[0], sents[1]), (sents[2], sents[3]), ],\n",
    "\n",
    "    # 此处参数同上\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    add_special_tokens=True,\n",
    "    max_length=30,\n",
    "    return_tensors=None,\n",
    "\n",
    "    # 返回新增的结果\n",
    "    return_token_type_ids=True,\n",
    "    return_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_special_tokens_mask=True,\n",
    "\n",
    "    #返回offset_mapping 标识每个词的起止位置,这个参数只能BertTokenizerFast使用\n",
    "    #return_offsets_mapping=True,\n",
    ")\n",
    "print(out)\n",
    "#input_ids: 就是编码后的词\n",
    "#token_type_ids: 第一个句子和特殊符号的位置是0,第二个句子的位置是1\n",
    "#special_tokens_mask: 特殊符号的位置是1,其他位置是0\n",
    "#attention_mask: pad的位置是0,其他位置是1\n",
    "#length: 返回句子长度\n",
    "\n",
    "tokenizer.decode(out['input_ids'][0])\n",
    "tokenizer.decode(out['input_ids'][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. vocab的操作"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "21128\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "zidian = tokenizer.get_vocab()\n",
    "\n",
    "print(type(zidian))\n",
    "print(len(zidian))\n",
    "print('月光' in zidian)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 添加token到字典"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "21131\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_tokens(new_tokens=['月光', '希望'])\n",
    "\n",
    "tokenizer.add_special_tokens({'eos_token':'[EOS]'})\n",
    "\n",
    "zidian = tokenizer.get_vocab()\n",
    "\n",
    "print(type(zidian))\n",
    "print(len(zidian))\n",
    "print('月光' in zidian)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 编码新词"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 21128, 3173, 21129, 21130, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[CLS] 月光 新 希望 [EOS] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer.encode(\n",
    "    text='月光新希望[EOS]',\n",
    "\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=15,\n",
    "    return_tensors=None\n",
    ")\n",
    "print(out)\n",
    "\n",
    "print(tokenizer.decode(out))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}