{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "from plotly import graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 标量求导"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-2.)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(0., requires_grad=True)\n",
    "a = 1\n",
    "b = -2\n",
    "c = 1\n",
    "y = a*torch.pow(x,2) + b*x+c\n",
    "\n",
    "y.backward()\n",
    "dy_dx = x.grad\n",
    "dy_dx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 非标量求导"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2., -2.],\n",
      "        [ 0.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "# 方法 1\n",
    "x = torch.tensor([[0., 0.],[1., 2.]], requires_grad=True)\n",
    "a = 1\n",
    "b = -2\n",
    "c = 1\n",
    "y = a*torch.pow(x,2) + b*x+c\n",
    "\n",
    "gradident = torch.ones(x.size())\n",
    "y.backward(gradient=gradident)\n",
    "x_grad1 = x.grad\n",
    "\n",
    "print(x_grad1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2., -2.],\n",
      "        [ 0.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "# 方法 2\n",
    "\n",
    "x = torch.tensor([[0., 0.],[1., 2.]], requires_grad=True)\n",
    "a = 1\n",
    "b = -2\n",
    "c = 1\n",
    "y = a*torch.pow(x,2) + b*x+c\n",
    "\n",
    "gradident = torch.ones(x.size())\n",
    "z = torch.sum(y*gradident)\n",
    "z.backward()\n",
    "x_grad2 = x.grad\n",
    "\n",
    "print(x_grad2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 autograd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一阶求导： tensor(-2.)\n",
      "二阶求导： tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(0., requires_grad=True)\n",
    "a = 1\n",
    "b = -2\n",
    "c = 1\n",
    "y = a*torch.pow(x,2) + b*x+c\n",
    "\n",
    "dy_dx = torch.autograd.grad(y,x,create_graph=True)[0]\n",
    "print('一阶求导：',dy_dx.data)\n",
    "\n",
    "dy2_dx2 = torch.autograd.grad(dy_dx,x,create_graph=True)[0]\n",
    "print('二阶求导：',dy2_dx2.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多自变量求导 dy1_dx1： tensor(2.)\n",
      "多自变量求导 dy2_dx1： tensor(1.)\n",
      "多因变量求导，输出结果为导数和 dy12_dx1： tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor(1., requires_grad=True)\n",
    "x2 = torch.tensor(2., requires_grad=True)\n",
    "\n",
    "y1 = x1 * x2\n",
    "y2 = x1 + x2\n",
    "\n",
    "(dy1_dx1, dy1_dx2) = torch.autograd.grad(outputs=y1, inputs=[x1,x2],retain_graph=True)\n",
    "print('多自变量求导 dy1_dx1：',dy1_dx1)\n",
    "\n",
    "(dy2_dx1, dy2_dx2) = torch.autograd.grad(outputs=y2, inputs=[x1,x2],retain_graph=True)\n",
    "print('多自变量求导 dy2_dx1：',dy2_dx1)\n",
    "\n",
    "(dy12_dx1, dy12_dx2) = torch.autograd.grad(outputs=[y1,y2], inputs=[x1,x2],retain_graph=True)\n",
    "print('多因变量求导，输出结果为导数和 dy12_dx1：', dy12_dx1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "利用SGD求最优值"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优值： tensor(0.) tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(0., requires_grad=True)\n",
    "a = 1\n",
    "b = -2\n",
    "c = 1\n",
    "def f(x):\n",
    "    return a*torch.pow(x,2) + b*x+c\n",
    "optimizer = torch.optim.SGD(params=[x],lr=0.01)\n",
    "\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print('最优值：',f(x).data, x.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 动态计算图"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.data tensor(13.4190)\n",
      "y_hat.data tensor([[-1.5928],\n",
      "        [ 0.7944],\n",
      "        [ 2.7251],\n",
      "        [ 5.8685],\n",
      "        [-1.7179],\n",
      "        [ 3.0768],\n",
      "        [ 5.2412],\n",
      "        [ 2.9496],\n",
      "        [ 0.6541],\n",
      "        [ 5.1317]])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([[3., 1.]], requires_grad=True)\n",
    "b = torch.tensor([[3. ]], requires_grad=True)\n",
    "X = torch.randn(10, 2)\n",
    "Y = torch.randn(10, 1)\n",
    "\n",
    "Y_hat = X@w.T + b\n",
    "loss = torch.mean(torch.pow(Y-Y_hat, 2))\n",
    "# 计算图在反向传播后被销毁\n",
    "loss.backward()\n",
    "\n",
    "# 需要保存计算图\n",
    "# loss.backward(retain_graph=True)\n",
    "print('loss.data',loss.data)\n",
    "print('y_hat.data',Y_hat.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}