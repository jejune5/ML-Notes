{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer pytorch\n",
    "![](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic2.zhimg.com%2Fv2-c2cb536d843168c1f44c39b8e0ec41d4_1440w.jpg%3Fsource%3D172ae18b&refer=http%3A%2F%2Fpic2.zhimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1657722307&t=19f124823ccd6e8bb467e1b6c3eb9703)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Encoder\n",
    "### 2.1 Positional Encoding\n",
    "使用sin、cos函数表示序列模型信息，将其**加上**（非拼接）原始词嵌入。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P E_{(p o s, 2 i)} &=\\sin \\left(p o s / 10000^{2 i / d_{\\text {model }}}\\right) \\\\\n",
    "P E_{(p o s, 2 i+1)} &=\\cos \\left(p o s / 10000^{2 i / d_{\\text {model }}}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "$pos$: token 位置，`[0, seq_len)`;\n",
    "$i$: Embedding向量维度序号 `[0, embedding_dim/2)`;\n",
    "$d_{\\text {model }$: 嵌入维度，`embedding_dim`；\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        d_model: 特征维度。\n",
    "        dropout: 默认值0.1。\n",
    "        max_len: 序列最大长度，默认值5k。\n",
    "    Inputs:\n",
    "        x: (batch_size, seq_len, embedding_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.pe = torch.zeros(1, max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.pow(10000, torch.arange(0, d_model, 2, dtype=torch.float32) / d_model)\n",
    "        self.pe[:, :, 0::2] = torch.sin(position / div_term)\n",
    "        self.pe[:, :, 1::2] = torch.cos(position / div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "        return self.dropout(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 10\n",
    "batch_size = 32\n",
    "d_model = 512\n",
    "PE = PositionalEncoding(d_model, 0)\n",
    "x = torch.zeros(batch_size, seq_len, d_model)\n",
    "y = PE(x)\n",
    "print(y.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 ScaledDotProductAttention\n",
    "- 步骤：\n",
    "    1. 计算`Q` 和 `K` 的分数`scores`。\n",
    "        $\n",
    "        \\mathbf{scores} = \\mathbf{Q} \\mathbf{K}^{\\top} / \\sqrt{d_{k}}\n",
    "        $\n",
    "\n",
    "    2. 掩蔽`scores`<PAD>后，做`softmax` 得到 `attn`\n",
    "        $\n",
    "            \\mathbf{attn}=softmax(mask(\\mathbf{scores}))\n",
    "        $\n",
    "\n",
    "    3. 计算`attn` 和 `V` 得到`context`。\n",
    "        $\n",
    "            \\mathbf{context}= \\mathbf{attn} \\cdot \\mathbf{K}\n",
    "        $\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        q: (batch_size, n_heads, len_q, d_k)\n",
    "        k: (batch_size, n_heads, len_k, d_k)\n",
    "        v: (batch_size, n_heads, len_v, d_v) , (len_v==len_k)\n",
    "        attn_mask: (batch_size, n_heads, len_q, len_k)\n",
    "    Returns:\n",
    "        context: (batch_size, n_heads, len_q, d_v)\n",
    "        attn: (batch_size, n_heads, len_k, d_k)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_k, dropout=0.1):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask):\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / (self.d_k ** (1 / 2))\n",
    "        if attn_mask is not None:\n",
    "            scores.masked_fill(attn_mask == 0, -1e9)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, v)\n",
    "        return context, attn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 MultiHeadAttention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        q : (batch_size, len_q, d_model)\n",
    "        k : (batch_size, len_k, d_model)\n",
    "        v : (batch_size, len_v, d_model) , (len_v==len_k)\n",
    "        attn_mask : (batch_size, n_heads, len_q, len_k)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, d_model, d_k, d_v, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_heads * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_heads * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_heads * d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(d_k=d_k)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        d_k, d_v, n_heads = self.d_k, self.d_v, self.n_head\n",
    "        batch_size, len_q, d_model = q.shape\n",
    "        len_k, len_v = k.shape[1], v.shape[1]\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        q = self.w_qs(q).view(batch_size, len_q, n_heads, d_k)\n",
    "        k = self.w_ks(k).view(batch_size, len_k, n_heads, d_k)\n",
    "        v = self.w_vs(v).view(batch_size, len_v, n_heads, d_v)\n",
    "\n",
    "        # 转成 ScaledDotProductAttention 输入形状 (batch_size, n_heads, len_q, d_k)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k = k.permute(0, 2, 1, 3)\n",
    "        v = v.permute(0, 2, 1, 3)\n",
    "\n",
    "        # 按 n_heads 维度复制\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "\n",
    "        context, attn = self.attention(q, k, v, attn_mask)\n",
    "        #   context: (batch_size, n_heads, len_q, d_v)\n",
    "        #   attn: (batch_size, n_heads, len_k, d_k)\n",
    "\n",
    "        context = context.permute(0, 2, 1, 3).view(batch_size, len_q, n_heads * d_v)\n",
    "\n",
    "        output = self.fc(context)\n",
    "        # output : (batch_size, len_q, d_model)\n",
    "\n",
    "        output = self.layernorm(output)\n",
    "\n",
    "        return output, attn\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-1.3416, -0.4472,  0.4472,  1.3416],\n       grad_fn=<NativeLayerNormBackward0>)"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4],dtype=torch.float)\n",
    "y = nn.LayerNorm(normalized_shape=4)(x)\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}