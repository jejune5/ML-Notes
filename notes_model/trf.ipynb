{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "tensor([[[ 33.3333,  68.8889,   3.3333,  ...,   6.6667,  72.2222,  92.2222],\n",
      "         [  0.0000,  62.2222, 105.5556,  ..., 108.8889,  18.8889,  26.6667],\n",
      "         [ 36.6667,  40.0000,  42.2222,  ...,  17.7778, 106.6667,  25.5556],\n",
      "         ...,\n",
      "         [ 30.0000,  88.8889,  57.7778,  ..., 101.1111,  43.3333,  75.5556],\n",
      "         [ 84.4445,  23.3333,  10.0000,  ...,   0.0000,  41.1111,  86.6667],\n",
      "         [ 54.4444,  81.1111,  71.1111,  ...,  76.6667,  13.3333,  88.8889]],\n",
      "\n",
      "        [[ 12.0461,   7.2670,  96.4531,  ...,   0.0000,   0.0000,  13.3333],\n",
      "         [ 35.3794, 108.3781,   8.6754,  ...,   7.7778,  85.5557,  26.6667],\n",
      "         [  5.3794,   0.0000,  40.8976,  ...,  18.8889,   0.0000,  10.0000],\n",
      "         ...,\n",
      "         [ 57.6016, 101.7115,  68.6754,  ...,  17.7778,  85.5557,   2.2222],\n",
      "         [ 57.6016,   0.0000,  80.8976,  ...,   0.0000,  63.3335,  43.3333],\n",
      "         [ 45.3794,  81.7115,   6.4531,  ...,  26.6667,  72.2223,   7.7778]],\n",
      "\n",
      "        [[ 87.6770,  30.6487,  77.7248,  ...,   0.0000,  37.7780,  38.8889],\n",
      "         [ 72.1214,  67.3154,  21.0581,  ..., 110.0000,  72.2225, 106.6667],\n",
      "         [ 78.7881,  42.8710,  69.9470,  ...,  68.8889,  80.0002,  16.6667],\n",
      "         ...,\n",
      "         [ 51.0103,  15.0932,  24.3915,  ...,  27.7778,  98.8891,   1.1111],\n",
      "         [ 94.3437,  42.8710,  74.3915,  ...,  86.6667,  95.5558,  22.2222],\n",
      "         [ 46.5659,  69.5376, 107.7248,  ...,  30.0000,  72.2225, 103.3333]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 48.2927,   9.8265,  40.0463,  ..., 103.3333,  31.1125,  51.1111],\n",
      "         [ 68.2927,  74.2710,  33.3797,  ...,  12.2222,   5.5570,  25.5556],\n",
      "         [ 46.0705,  77.6043,   0.0000,  ...,  84.4445,  25.5570,  55.5556],\n",
      "         ...,\n",
      "         [ 46.0705,  26.4932,   7.8241,  ...,   0.0000,  11.1125,  15.5556],\n",
      "         [ 28.2927,   0.0000,  70.0463,  ...,  65.5556,  46.6681,   3.3333],\n",
      "         [ 36.0705,  65.3821,  18.9352,  ...,   6.6667,  30.0014,   5.5556]],\n",
      "\n",
      "        [[ 46.0224, 109.8972,  18.5177,  ...,  36.6667,  64.4460,   8.8889],\n",
      "         [ 71.5780,  32.1194,  77.4066,  ...,  45.5556,  10.0015,  64.4445],\n",
      "         [ 86.0224,  95.4527,  59.6288,  ...,  83.3333,  26.6682,  70.0000],\n",
      "         ...,\n",
      "         [ 61.5780,  84.3416,  90.7400,  ...,  74.4445, 106.6682,  63.3333],\n",
      "         [ 29.3557,   0.0000,  64.0733,  ...,  32.2222,  73.3349,  46.6667],\n",
      "         [ 91.5780,  32.1194,  74.0733,  ...,  53.3333,  83.3349,  52.2222]],\n",
      "\n",
      "        [[ 68.8785,  87.9297,  68.4050,  ...,  30.0000,  82.2239,  13.3333],\n",
      "         [  0.0000, 106.8186, 108.4050,  ...,  64.4445,  18.8905,   5.5556],\n",
      "         [ 85.5451,  80.1519,  71.7384,  ...,   1.1111,   5.5572,  46.6667],\n",
      "         ...,\n",
      "         [ 32.2118,   0.0000,  52.8495,  ...,  28.8889,  32.2239, 103.3333],\n",
      "         [  0.0000,  15.7075,  67.2939,  ...,   3.3333,  45.5572,  30.0000],\n",
      "         [ 68.8785,   0.0000,  37.2939,  ...,  73.3333,  87.7794,  38.8889]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        '''\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "x = torch.randint(100, (15, 32, 300))\n",
    "pe = PositionalEncoding(300)\n",
    "y_1 = pe(x)\n",
    "\n",
    "print('-' * 10)\n",
    "# x.shape\n",
    "# print(x)\n",
    "print(y_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 512])\n",
      "torch.Size([10, 1, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([10, 32, 512])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(6)\n",
    "\n",
    "\n",
    "class PositionalEncoding2(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding2, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.pow(10000, torch.arange(0, d_model, 2, dtype=torch.float32) / d_model)\n",
    "        # print(div_term)\n",
    "        # div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position / div_term)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        print(pe.shape)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        '''\n",
    "        print(self.pe.shape)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "max_len = 10\n",
    "batch_size = 32\n",
    "d_model = 512\n",
    "PE2 = PositionalEncoding2(d_model, max_len=max_len)\n",
    "x = torch.zeros(max_len, batch_size, d_model)\n",
    "y = PE2(x)\n",
    "y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 1.3333e+01,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00, -4.0000e+01,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         ...,\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  5.5556e+01,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]],\n\n        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  9.8889e+01,\n           8.3333e+01,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  9.3934e+01,  0.0000e+00,  ...,  0.0000e+00,\n           7.2222e+01,  0.0000e+00],\n         ...,\n         [-5.7602e+01,  0.0000e+00, -6.8675e+01,  ..., -1.7778e+01,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  6.7267e+01, -8.0898e+01,  ...,  6.3333e+01,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.6667e+01,\n           0.0000e+00,  0.0000e+00]],\n\n        [[-8.7677e+01,  0.0000e+00, -7.7725e+01,  ...,  1.0000e+02,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.1000e+02,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00, -4.2871e+01,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         ...,\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00, -4.2871e+01,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]],\n\n        ...,\n\n        [[-4.8293e+01,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          -3.1113e+01,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.2222e+01,\n           0.0000e+00, -2.5556e+01],\n         [ 0.0000e+00,  0.0000e+00,  1.5602e+01,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         ...,\n         [ 0.0000e+00,  0.0000e+00,  4.7684e-07,  ...,  1.0111e+02,\n          -1.1113e+01, -1.5556e+01],\n         [ 0.0000e+00,  8.2049e+01,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00, -5.5556e+00]],\n\n        [[ 0.0000e+00,  0.0000e+00, -1.8518e+01,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         ...,\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [-9.1578e+01, -3.2119e+01,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00, -5.2222e+01]],\n\n        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.0000e+01,\n           0.0000e+00,  0.0000e+00],\n         [ 9.3323e+01,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          -5.5572e+00,  0.0000e+00],\n         ...,\n         [ 0.0000e+00,  8.7930e+01,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00, -1.0333e+02],\n         [ 1.0666e+02,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  7.9297e+00,  0.0000e+00,  ...,  0.0000e+00,\n          -8.7779e+01,  0.0000e+00]]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2 - y_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50])\n",
      "torch.Size([50])\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.3842e-07,\n",
      "        -2.3842e-07,  0.0000e+00,  0.0000e+00, -4.7684e-07,  0.0000e+00,\n",
      "        -4.7684e-07,  0.0000e+00,  0.0000e+00, -1.9073e-06,  0.0000e+00,\n",
      "        -9.5367e-07, -3.8147e-06,  0.0000e+00, -1.9073e-06, -3.8147e-06,\n",
      "        -7.6294e-06, -1.5259e-05, -1.1444e-05, -7.6294e-06,  0.0000e+00,\n",
      "         0.0000e+00, -3.8147e-05, -3.0518e-05,  0.0000e+00, -7.6294e-05,\n",
      "        -3.0518e-05,  0.0000e+00, -1.2207e-04, -6.1035e-05,  0.0000e+00,\n",
      "        -1.8311e-04, -1.2207e-04,  6.1035e-05, -2.4414e-04, -8.5449e-04,\n",
      "        -7.3242e-04, -4.8828e-04, -1.2207e-03, -9.7656e-04, -4.8828e-04,\n",
      "        -2.1973e-03, -1.4648e-03, -3.4180e-03, -4.8828e-04,  1.9531e-03])\n"
     ]
    }
   ],
   "source": [
    "d_model = 100\n",
    "\n",
    "div_term = torch.pow(10000, torch.arange(0, d_model, 2).float() / d_model)\n",
    "print(div_term.shape)\n",
    "div_term2 = 1 / torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "print(div_term2.shape)\n",
    "\n",
    "print(div_term - div_term2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于在 Encoder 和 Decoder 中都需要进行 mask 操作，因此就无法确定这个函数的参数中 seq_len 的值，如果是在 Encoder 中调用的，seq_len 就等于 src_len；如果是在 Decoder 中调用的，seq_len 就有可能等于 src_len，也有可能等于 tgt_len（因为 Decoder 有两次 mask）\n",
    "\n",
    "这个函数最核心的一句代码是 seq_k.data.eq(0)，这句的作用是返回一个大小和 seq_k 一样的 tensor，只不过里面的值只有 True 和 False。如果 seq_k 某个位置的值等于 0，那么对应位置就是 True，否则即为 False。举个例子，输入为 seq_data = [1, 2, 3, 4, 0]，seq_data.data.eq(0) 就会返回 [False, False, False, False, True]\n",
    "\n",
    "剩下的代码主要是扩展维度，强烈建议读者打印出来，看看最终返回的数据是什么样子"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    '''\n",
    "    seq_q: [batch_size, seq_len]\n",
    "    seq_k: [batch_size, seq_len]\n",
    "    seq_len could be src_len or it could be tgt_len\n",
    "    seq_len in seq_q and seq_len in seq_k maybe not equal\n",
    "    '''\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # [batch_size, 1, len_k], True is masked\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False,  True, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[ True, False, False, False, False,  True, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]],\n\n        [[False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False]]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_k = torch.randint(100, (32, 15))\n",
    "seq_k.eq(0).unsqueeze(1)\n",
    "# seq_k.data.eq(0).unsqueeze(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_attn_subsequence_mask(seq):\n",
    "    '''\n",
    "    seq: [batch_size, tgt_len]\n",
    "    '''\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1)  # Upper triangular matrix\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\n",
    "    return subsequence_mask  # [batch_size, tgt_len, tgt_len]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.nn.modules.transformer.TransformerDecoder"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "nn.TransformerDecoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}