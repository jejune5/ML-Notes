{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "\n",
    "def mnist_dataset():\n",
    "    (x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    #normalize\n",
    "    x = x/255.0\n",
    "    x_test = x_test/255.0\n",
    "    \n",
    "    return (x, y), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(list(zip([1, 2, 3, 4], ['a', 'b', 'c', 'd'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(60000, 28, 28)\n<tf.Variable 'W1:0' shape=(784, 1000) dtype=float32, numpy=\narray([[-0.05548986,  0.0535214 , -0.00169814, ...,  0.00753841,\n         0.02009938, -0.04883188],\n       [ 0.0176485 ,  0.00353666, -0.00088965, ..., -0.0524257 ,\n         0.0347659 ,  0.05168242],\n       [-0.00562605,  0.05685939, -0.00731237, ...,  0.01945076,\n         0.00609486, -0.04025821],\n       ...,\n       [ 0.0319755 ,  0.0302523 , -0.03683082, ..., -0.01378015,\n         0.01328975,  0.0066792 ],\n       [ 0.05404182, -0.02128858,  0.05237038, ..., -0.03639896,\n        -0.01706782,  0.03724303],\n       [-0.02343996,  0.02638258, -0.02924831, ..., -0.04192679,\n        -0.03086094,  0.05391714]], dtype=float32)>\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "(x, y), (x_test, y_test) = mnist_dataset()\n",
    "print(x.shape)\n",
    "W1 = tf.compat.v1.get_variable(shape=(784, 1000), name='W1')\n",
    "print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class myModel:\n",
    "    def __init__(self):\n",
    "        ####################\n",
    "        '''声明模型对应的参数'''\n",
    "        ####################\n",
    "        self.W1=tf.compat.v1.get_variable(shape=(784,1000),name=\"W1\",initializer=tf.compat.v1.random_normal_initializer)\n",
    "        self.b1=tf.compat.v1.get_variable(shape=(1,1000),name=\"b1\",initializer=tf.compat.v1.random_normal_initializer)\n",
    "        self.W2=tf.compat.v1.get_variable(shape=(1000,666),name=\"W2\",initializer=tf.compat.v1.random_normal_initializer)\n",
    "        self.b2=tf.compat.v1.get_variable(shape=(1,666),name=\"b2\",initializer=tf.compat.v1.random_normal_initializer)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        ####################\n",
    "        '''实现模型函数体，返回未归一化的logits'''\n",
    "        ####################\n",
    "        x=tf.reshape(x,(-1,784))\n",
    "        hidden = tf.nn.relu(tf.matmul(x,self.W1)+self.b1)\n",
    "        logits = tf.nn.relu(tf.matmul(hidden,self.W2)+self.b2)\n",
    "        return logits\n",
    "        \n",
    "model = myModel()\n",
    "\n",
    "optimizer = optimizers.Adam()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels))\n",
    "\n",
    "@tf.function\n",
    "def compute_accuracy(logits, labels):\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(logits, y)\n",
    "\n",
    "    # compute gradient\n",
    "    trainable_vars = [model.W1, model.W2, model.b1, model.b2]\n",
    "    grads = tape.gradient(loss, trainable_vars)\n",
    "    for g, v in zip(grads, trainable_vars):\n",
    "        v.assign_sub(0.01*g)\n",
    "\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "\n",
    "    # loss and accuracy is scalar tensor\n",
    "    return loss, accuracy\n",
    "\n",
    "@tf.function\n",
    "def test(model, x, y):\n",
    "    logits = model(x)\n",
    "    loss = compute_loss(logits, y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "epoch 0 : loss 94.984535 ; accuracy 0.45751667\n",
      "epoch 1 : loss 93.95316 ; accuracy 0.46193334\n",
      "epoch 2 : loss 92.9443 ; accuracy 0.46648332\n",
      "epoch 3 : loss 91.95789 ; accuracy 0.4711\n",
      "epoch 4 : loss 90.98973 ; accuracy 0.47515\n",
      "epoch 5 : loss 90.03605 ; accuracy 0.47956666\n",
      "epoch 6 : loss 89.093 ; accuracy 0.48335\n",
      "epoch 7 : loss 88.15199 ; accuracy 0.48678333\n",
      "epoch 8 : loss 87.2116 ; accuracy 0.49061668\n",
      "epoch 9 : loss 86.26929 ; accuracy 0.4943\n",
      "epoch 10 : loss 85.322174 ; accuracy 0.49751666\n",
      "epoch 11 : loss 84.36929 ; accuracy 0.50115\n",
      "epoch 12 : loss 83.402115 ; accuracy 0.50371665\n",
      "epoch 13 : loss 82.421585 ; accuracy 0.5069\n",
      "epoch 14 : loss 81.42164 ; accuracy 0.50995\n",
      "epoch 15 : loss 80.398865 ; accuracy 0.5129667\n",
      "epoch 16 : loss 79.3523 ; accuracy 0.51596665\n",
      "epoch 17 : loss 78.26984 ; accuracy 0.51926666\n",
      "epoch 18 : loss 77.15791 ; accuracy 0.52235\n",
      "epoch 19 : loss 76.00071 ; accuracy 0.5258833\n",
      "epoch 20 : loss 74.78045 ; accuracy 0.5291\n",
      "epoch 21 : loss 73.4809 ; accuracy 0.5323333\n",
      "epoch 22 : loss 72.08673 ; accuracy 0.5358\n",
      "epoch 23 : loss 70.55413 ; accuracy 0.53943336\n",
      "epoch 24 : loss 68.82091 ; accuracy 0.54323334\n",
      "epoch 25 : loss 66.812744 ; accuracy 0.5474\n",
      "epoch 26 : loss 64.48492 ; accuracy 0.5510333\n",
      "epoch 27 : loss 61.88258 ; accuracy 0.5557333\n",
      "epoch 28 : loss 59.14223 ; accuracy 0.56028336\n",
      "epoch 29 : loss 56.405075 ; accuracy 0.5649833\n",
      "epoch 30 : loss 53.800938 ; accuracy 0.56983334\n",
      "epoch 31 : loss 51.404564 ; accuracy 0.5746667\n",
      "epoch 32 : loss 49.232056 ; accuracy 0.57878333\n",
      "epoch 33 : loss 47.31186 ; accuracy 0.58348334\n",
      "epoch 34 : loss 45.651268 ; accuracy 0.5885\n",
      "epoch 35 : loss 44.230442 ; accuracy 0.5938\n",
      "epoch 36 : loss 43.030754 ; accuracy 0.59921664\n",
      "epoch 37 : loss 42.021385 ; accuracy 0.6049\n",
      "epoch 38 : loss 41.170277 ; accuracy 0.61081666\n",
      "epoch 39 : loss 40.444035 ; accuracy 0.61625\n",
      "epoch 40 : loss 39.80159 ; accuracy 0.62166667\n",
      "epoch 41 : loss 39.220142 ; accuracy 0.6263667\n",
      "epoch 42 : loss 38.68386 ; accuracy 0.63063335\n",
      "epoch 43 : loss 38.1841 ; accuracy 0.63448334\n",
      "epoch 44 : loss 37.716576 ; accuracy 0.63808334\n",
      "epoch 45 : loss 37.275208 ; accuracy 0.6415333\n",
      "epoch 46 : loss 36.8569 ; accuracy 0.64486665\n",
      "epoch 47 : loss 36.459866 ; accuracy 0.64865\n",
      "epoch 48 : loss 36.080933 ; accuracy 0.6516\n",
      "epoch 49 : loss 35.717945 ; accuracy 0.65426666\n",
      "epoch 50 : loss 35.369484 ; accuracy 0.6572667\n",
      "epoch 51 : loss 35.034904 ; accuracy 0.65996665\n",
      "epoch 52 : loss 34.71438 ; accuracy 0.66263336\n",
      "epoch 53 : loss 34.406914 ; accuracy 0.6655\n",
      "epoch 54 : loss 34.11109 ; accuracy 0.6678\n",
      "epoch 55 : loss 33.82597 ; accuracy 0.6702833\n",
      "epoch 56 : loss 33.551193 ; accuracy 0.6727833\n",
      "epoch 57 : loss 33.286297 ; accuracy 0.6753\n",
      "epoch 58 : loss 33.03062 ; accuracy 0.6773833\n",
      "epoch 59 : loss 32.783413 ; accuracy 0.6795667\n",
      "epoch 60 : loss 32.544144 ; accuracy 0.68168336\n",
      "epoch 61 : loss 32.312153 ; accuracy 0.6836\n",
      "epoch 62 : loss 32.08714 ; accuracy 0.6854333\n",
      "epoch 63 : loss 31.868605 ; accuracy 0.68743336\n",
      "epoch 64 : loss 31.656185 ; accuracy 0.68906665\n",
      "epoch 65 : loss 31.449488 ; accuracy 0.6907333\n",
      "epoch 66 : loss 31.248098 ; accuracy 0.69245\n",
      "epoch 67 : loss 31.051937 ; accuracy 0.6944\n",
      "epoch 68 : loss 30.860872 ; accuracy 0.6960833\n",
      "epoch 69 : loss 30.674597 ; accuracy 0.6975333\n",
      "epoch 70 : loss 30.492697 ; accuracy 0.6990833\n",
      "epoch 71 : loss 30.315044 ; accuracy 0.70075\n",
      "epoch 72 : loss 30.141586 ; accuracy 0.7021\n",
      "epoch 73 : loss 29.972092 ; accuracy 0.7037\n",
      "epoch 74 : loss 29.806396 ; accuracy 0.7054167\n",
      "epoch 75 : loss 29.644327 ; accuracy 0.70673335\n",
      "epoch 76 : loss 29.485653 ; accuracy 0.70818335\n",
      "epoch 77 : loss 29.330183 ; accuracy 0.7097\n",
      "epoch 78 : loss 29.177805 ; accuracy 0.7111667\n",
      "epoch 79 : loss 29.028334 ; accuracy 0.71248335\n",
      "epoch 80 : loss 28.881697 ; accuracy 0.71391666\n",
      "epoch 81 : loss 28.737906 ; accuracy 0.71508336\n",
      "epoch 82 : loss 28.596937 ; accuracy 0.71635\n",
      "epoch 83 : loss 28.4586 ; accuracy 0.71748334\n",
      "epoch 84 : loss 28.322773 ; accuracy 0.71856666\n",
      "epoch 85 : loss 28.18944 ; accuracy 0.7197\n",
      "epoch 86 : loss 28.058546 ; accuracy 0.72105\n",
      "epoch 87 : loss 27.929976 ; accuracy 0.7221\n",
      "epoch 88 : loss 27.803555 ; accuracy 0.7233833\n",
      "epoch 89 : loss 27.679094 ; accuracy 0.7241833\n",
      "epoch 90 : loss 27.556442 ; accuracy 0.72495\n",
      "epoch 91 : loss 27.43552 ; accuracy 0.7261\n",
      "epoch 92 : loss 27.316355 ; accuracy 0.72725\n",
      "epoch 93 : loss 27.198986 ; accuracy 0.72825\n",
      "epoch 94 : loss 27.083462 ; accuracy 0.7292333\n",
      "epoch 95 : loss 26.969746 ; accuracy 0.73008335\n",
      "epoch 96 : loss 26.857712 ; accuracy 0.7312667\n",
      "epoch 97 : loss 26.747292 ; accuracy 0.73215\n",
      "epoch 98 : loss 26.638428 ; accuracy 0.7330667\n",
      "epoch 99 : loss 26.531084 ; accuracy 0.73406667\n",
      "epoch 100 : loss 26.425259 ; accuracy 0.73501664\n",
      "epoch 101 : loss 26.320932 ; accuracy 0.7360333\n",
      "epoch 102 : loss 26.218075 ; accuracy 0.73693335\n",
      "epoch 103 : loss 26.116625 ; accuracy 0.73756665\n",
      "epoch 104 : loss 26.016521 ; accuracy 0.7383\n",
      "epoch 105 : loss 25.917707 ; accuracy 0.7392167\n",
      "epoch 106 : loss 25.820133 ; accuracy 0.7399\n",
      "epoch 107 : loss 25.723757 ; accuracy 0.74083334\n",
      "epoch 108 : loss 25.62855 ; accuracy 0.74148333\n",
      "epoch 109 : loss 25.534512 ; accuracy 0.74258333\n",
      "epoch 110 : loss 25.441639 ; accuracy 0.7434\n",
      "epoch 111 : loss 25.349874 ; accuracy 0.74445\n",
      "epoch 112 : loss 25.259167 ; accuracy 0.7450167\n",
      "epoch 113 : loss 25.169483 ; accuracy 0.74556667\n",
      "epoch 114 : loss 25.080788 ; accuracy 0.74623334\n",
      "epoch 115 : loss 24.993042 ; accuracy 0.74698335\n",
      "epoch 116 : loss 24.906218 ; accuracy 0.7475833\n",
      "epoch 117 : loss 24.820297 ; accuracy 0.74826664\n",
      "epoch 118 : loss 24.735287 ; accuracy 0.7488667\n",
      "epoch 119 : loss 24.651201 ; accuracy 0.74945\n",
      "epoch 120 : loss 24.56805 ; accuracy 0.75025\n",
      "epoch 121 : loss 24.485825 ; accuracy 0.751\n",
      "epoch 122 : loss 24.404528 ; accuracy 0.75175\n",
      "epoch 123 : loss 24.324108 ; accuracy 0.7524833\n",
      "epoch 124 : loss 24.244526 ; accuracy 0.75303334\n",
      "epoch 125 : loss 24.16575 ; accuracy 0.7536167\n",
      "epoch 126 : loss 24.087738 ; accuracy 0.75405\n",
      "epoch 127 : loss 24.010483 ; accuracy 0.7547333\n",
      "epoch 128 : loss 23.933987 ; accuracy 0.7554333\n",
      "epoch 129 : loss 23.858238 ; accuracy 0.75603336\n",
      "epoch 130 : loss 23.78322 ; accuracy 0.75658333\n",
      "epoch 131 : loss 23.708908 ; accuracy 0.75706667\n",
      "epoch 132 : loss 23.635303 ; accuracy 0.75755\n",
      "epoch 133 : loss 23.5624 ; accuracy 0.7583333\n",
      "epoch 134 : loss 23.4902 ; accuracy 0.7589\n",
      "epoch 135 : loss 23.418695 ; accuracy 0.7593167\n",
      "epoch 136 : loss 23.347887 ; accuracy 0.7599667\n",
      "epoch 137 : loss 23.27775 ; accuracy 0.7604667\n",
      "epoch 138 : loss 23.208275 ; accuracy 0.76121664\n",
      "epoch 139 : loss 23.139446 ; accuracy 0.76175\n",
      "epoch 140 : loss 23.07125 ; accuracy 0.76203334\n",
      "epoch 141 : loss 23.00368 ; accuracy 0.7626167\n",
      "epoch 142 : loss 22.936718 ; accuracy 0.7632833\n",
      "epoch 143 : loss 22.870352 ; accuracy 0.76381665\n",
      "epoch 144 : loss 22.804575 ; accuracy 0.7645\n",
      "epoch 145 : loss 22.739374 ; accuracy 0.7651\n",
      "epoch 146 : loss 22.674728 ; accuracy 0.76563334\n",
      "epoch 147 : loss 22.610626 ; accuracy 0.7661333\n",
      "epoch 148 : loss 22.54705 ; accuracy 0.76661664\n",
      "epoch 149 : loss 22.483995 ; accuracy 0.7672\n",
      "epoch 150 : loss 22.421467 ; accuracy 0.7676167\n",
      "epoch 151 : loss 22.35947 ; accuracy 0.76823336\n",
      "epoch 152 : loss 22.29799 ; accuracy 0.76856667\n",
      "epoch 153 : loss 22.237038 ; accuracy 0.76918334\n",
      "epoch 154 : loss 22.176624 ; accuracy 0.76975\n",
      "epoch 155 : loss 22.116766 ; accuracy 0.77031666\n",
      "epoch 156 : loss 22.057463 ; accuracy 0.7709\n",
      "epoch 157 : loss 21.998705 ; accuracy 0.77141666\n",
      "epoch 158 : loss 21.94049 ; accuracy 0.77183336\n",
      "epoch 159 : loss 21.882805 ; accuracy 0.77245\n",
      "epoch 160 : loss 21.825644 ; accuracy 0.773\n",
      "epoch 161 : loss 21.768997 ; accuracy 0.77355\n",
      "epoch 162 : loss 21.712856 ; accuracy 0.7740167\n",
      "epoch 163 : loss 21.657179 ; accuracy 0.7743833\n",
      "epoch 164 : loss 21.601955 ; accuracy 0.7748\n",
      "epoch 165 : loss 21.547167 ; accuracy 0.77521664\n",
      "epoch 166 : loss 21.492802 ; accuracy 0.77565\n",
      "epoch 167 : loss 21.43886 ; accuracy 0.7761667\n",
      "epoch 168 : loss 21.385324 ; accuracy 0.7767\n",
      "epoch 169 : loss 21.332191 ; accuracy 0.7770333\n",
      "epoch 170 : loss 21.27945 ; accuracy 0.7775\n",
      "epoch 171 : loss 21.227085 ; accuracy 0.7777\n",
      "epoch 172 : loss 21.1751 ; accuracy 0.7780667\n",
      "epoch 173 : loss 21.123487 ; accuracy 0.77856666\n",
      "epoch 174 : loss 21.07224 ; accuracy 0.77898335\n",
      "epoch 175 : loss 21.021364 ; accuracy 0.7794333\n",
      "epoch 176 : loss 20.97085 ; accuracy 0.7797667\n",
      "epoch 177 : loss 20.920696 ; accuracy 0.7801167\n",
      "epoch 178 : loss 20.870909 ; accuracy 0.7804667\n",
      "epoch 179 : loss 20.821487 ; accuracy 0.7809167\n",
      "epoch 180 : loss 20.772438 ; accuracy 0.7812\n",
      "epoch 181 : loss 20.723755 ; accuracy 0.7818\n",
      "epoch 182 : loss 20.675426 ; accuracy 0.78223336\n",
      "epoch 183 : loss 20.627459 ; accuracy 0.7827\n",
      "epoch 184 : loss 20.579844 ; accuracy 0.78321666\n",
      "epoch 185 : loss 20.532585 ; accuracy 0.7836667\n",
      "epoch 186 : loss 20.485676 ; accuracy 0.7841667\n",
      "epoch 187 : loss 20.439106 ; accuracy 0.78456664\n",
      "epoch 188 : loss 20.392883 ; accuracy 0.7848833\n",
      "epoch 189 : loss 20.346996 ; accuracy 0.78505\n",
      "epoch 190 : loss 20.301437 ; accuracy 0.7856\n",
      "epoch 191 : loss 20.256205 ; accuracy 0.7859833\n",
      "epoch 192 : loss 20.211288 ; accuracy 0.7862\n",
      "epoch 193 : loss 20.166672 ; accuracy 0.7866167\n",
      "epoch 194 : loss 20.122355 ; accuracy 0.78685\n",
      "epoch 195 : loss 20.078335 ; accuracy 0.78711665\n",
      "epoch 196 : loss 20.03461 ; accuracy 0.7873333\n",
      "epoch 197 : loss 19.991175 ; accuracy 0.7876667\n",
      "epoch 198 : loss 19.948019 ; accuracy 0.7880833\n",
      "epoch 199 : loss 19.905146 ; accuracy 0.7884333\n",
      "epoch 200 : loss 19.862545 ; accuracy 0.7887667\n",
      "epoch 201 : loss 19.820208 ; accuracy 0.7891667\n",
      "epoch 202 : loss 19.778137 ; accuracy 0.7894833\n",
      "epoch 203 : loss 19.736324 ; accuracy 0.78975\n",
      "epoch 204 : loss 19.69477 ; accuracy 0.78996664\n",
      "epoch 205 : loss 19.653479 ; accuracy 0.79025\n",
      "epoch 206 : loss 19.612446 ; accuracy 0.79055\n",
      "epoch 207 : loss 19.57168 ; accuracy 0.7909833\n",
      "epoch 208 : loss 19.531178 ; accuracy 0.79125\n",
      "epoch 209 : loss 19.49093 ; accuracy 0.79158336\n",
      "epoch 210 : loss 19.450943 ; accuracy 0.792\n",
      "epoch 211 : loss 19.411213 ; accuracy 0.79228336\n",
      "epoch 212 : loss 19.371733 ; accuracy 0.7926667\n",
      "epoch 213 : loss 19.332499 ; accuracy 0.79293334\n",
      "epoch 214 : loss 19.293505 ; accuracy 0.7932\n",
      "epoch 215 : loss 19.254744 ; accuracy 0.7934667\n",
      "epoch 216 : loss 19.216213 ; accuracy 0.7938\n",
      "epoch 217 : loss 19.177906 ; accuracy 0.79405\n",
      "epoch 218 : loss 19.139826 ; accuracy 0.7945167\n",
      "epoch 219 : loss 19.101969 ; accuracy 0.79483336\n",
      "epoch 220 : loss 19.064342 ; accuracy 0.7951\n",
      "epoch 221 : loss 19.026945 ; accuracy 0.79546666\n",
      "epoch 222 : loss 18.98977 ; accuracy 0.79578334\n",
      "epoch 223 : loss 18.952818 ; accuracy 0.79611665\n",
      "epoch 224 : loss 18.916088 ; accuracy 0.79658335\n",
      "epoch 225 : loss 18.879587 ; accuracy 0.79693335\n",
      "epoch 226 : loss 18.843306 ; accuracy 0.7972\n",
      "epoch 227 : loss 18.807245 ; accuracy 0.79745\n",
      "epoch 228 : loss 18.771408 ; accuracy 0.7977\n",
      "epoch 229 : loss 18.735783 ; accuracy 0.798\n",
      "epoch 230 : loss 18.70038 ; accuracy 0.79833335\n",
      "epoch 231 : loss 18.665184 ; accuracy 0.79873335\n",
      "epoch 232 : loss 18.630192 ; accuracy 0.79908335\n",
      "epoch 233 : loss 18.5954 ; accuracy 0.79935\n",
      "epoch 234 : loss 18.560812 ; accuracy 0.7996333\n",
      "epoch 235 : loss 18.526417 ; accuracy 0.7999833\n",
      "epoch 236 : loss 18.49222 ; accuracy 0.80015\n",
      "epoch 237 : loss 18.458216 ; accuracy 0.8003833\n",
      "epoch 238 : loss 18.424408 ; accuracy 0.8006833\n",
      "epoch 239 : loss 18.390793 ; accuracy 0.8010833\n",
      "epoch 240 : loss 18.357359 ; accuracy 0.8015\n",
      "epoch 241 : loss 18.324108 ; accuracy 0.8017\n",
      "epoch 242 : loss 18.291048 ; accuracy 0.8020833\n",
      "epoch 243 : loss 18.25817 ; accuracy 0.80228335\n",
      "epoch 244 : loss 18.225471 ; accuracy 0.8025333\n",
      "epoch 245 : loss 18.192955 ; accuracy 0.8029\n",
      "epoch 246 : loss 18.160616 ; accuracy 0.8032167\n",
      "epoch 247 : loss 18.128468 ; accuracy 0.80343336\n",
      "epoch 248 : loss 18.096504 ; accuracy 0.8038\n",
      "epoch 249 : loss 18.064716 ; accuracy 0.80406666\n",
      "epoch 250 : loss 18.033115 ; accuracy 0.8043333\n",
      "epoch 251 : loss 18.001694 ; accuracy 0.80473334\n",
      "epoch 252 : loss 17.970463 ; accuracy 0.80515\n",
      "epoch 253 : loss 17.939411 ; accuracy 0.8056\n",
      "epoch 254 : loss 17.90854 ; accuracy 0.80583334\n",
      "epoch 255 : loss 17.87785 ; accuracy 0.80615\n",
      "epoch 256 : loss 17.84733 ; accuracy 0.80655\n",
      "epoch 257 : loss 17.816978 ; accuracy 0.80686665\n",
      "epoch 258 : loss 17.78679 ; accuracy 0.80705\n",
      "epoch 259 : loss 17.756775 ; accuracy 0.8072\n",
      "epoch 260 : loss 17.726915 ; accuracy 0.80758333\n",
      "epoch 261 : loss 17.697216 ; accuracy 0.8078833\n",
      "epoch 262 : loss 17.667677 ; accuracy 0.8081333\n",
      "epoch 263 : loss 17.638292 ; accuracy 0.80833334\n",
      "epoch 264 : loss 17.609055 ; accuracy 0.8085333\n",
      "epoch 265 : loss 17.57997 ; accuracy 0.80873334\n",
      "epoch 266 : loss 17.55103 ; accuracy 0.8089667\n",
      "epoch 267 : loss 17.522238 ; accuracy 0.80925\n",
      "epoch 268 : loss 17.493595 ; accuracy 0.80946666\n",
      "epoch 269 : loss 17.465105 ; accuracy 0.80971664\n",
      "epoch 270 : loss 17.43677 ; accuracy 0.81005\n",
      "epoch 271 : loss 17.408585 ; accuracy 0.81041664\n",
      "epoch 272 : loss 17.380556 ; accuracy 0.8107167\n",
      "epoch 273 : loss 17.352682 ; accuracy 0.81091666\n",
      "epoch 274 : loss 17.324951 ; accuracy 0.81118333\n",
      "epoch 275 : loss 17.297377 ; accuracy 0.8113833\n",
      "epoch 276 : loss 17.269945 ; accuracy 0.81166667\n",
      "epoch 277 : loss 17.242659 ; accuracy 0.8119\n",
      "epoch 278 : loss 17.215519 ; accuracy 0.81221664\n",
      "epoch 279 : loss 17.188522 ; accuracy 0.8124\n",
      "epoch 280 : loss 17.161669 ; accuracy 0.8126\n",
      "epoch 281 : loss 17.134958 ; accuracy 0.8128333\n",
      "epoch 282 : loss 17.108381 ; accuracy 0.81303334\n",
      "epoch 283 : loss 17.081942 ; accuracy 0.8131833\n",
      "epoch 284 : loss 17.055634 ; accuracy 0.81341666\n",
      "epoch 285 : loss 17.02946 ; accuracy 0.8136333\n",
      "epoch 286 : loss 17.003414 ; accuracy 0.8138667\n",
      "epoch 287 : loss 16.977493 ; accuracy 0.81413335\n",
      "epoch 288 : loss 16.951698 ; accuracy 0.8143\n",
      "epoch 289 : loss 16.926031 ; accuracy 0.81451666\n",
      "epoch 290 : loss 16.900484 ; accuracy 0.8147333\n",
      "epoch 291 : loss 16.87506 ; accuracy 0.8150667\n",
      "epoch 292 : loss 16.849758 ; accuracy 0.8153333\n",
      "epoch 293 : loss 16.82458 ; accuracy 0.81563336\n",
      "epoch 294 : loss 16.799522 ; accuracy 0.8157833\n",
      "epoch 295 : loss 16.774584 ; accuracy 0.81596667\n",
      "epoch 296 : loss 16.749767 ; accuracy 0.8161\n",
      "epoch 297 : loss 16.725069 ; accuracy 0.81635\n",
      "epoch 298 : loss 16.700487 ; accuracy 0.81663334\n",
      "epoch 299 : loss 16.676025 ; accuracy 0.81698334\n",
      "epoch 300 : loss 16.651676 ; accuracy 0.8171333\n",
      "epoch 301 : loss 16.627445 ; accuracy 0.8176\n",
      "epoch 302 : loss 16.60333 ; accuracy 0.8178167\n",
      "epoch 303 : loss 16.57933 ; accuracy 0.81796664\n",
      "epoch 304 : loss 16.555439 ; accuracy 0.8181\n",
      "epoch 305 : loss 16.531662 ; accuracy 0.81831664\n",
      "epoch 306 : loss 16.507994 ; accuracy 0.81845\n",
      "epoch 307 : loss 16.484436 ; accuracy 0.81873333\n",
      "epoch 308 : loss 16.460987 ; accuracy 0.8189333\n",
      "epoch 309 : loss 16.437647 ; accuracy 0.81911665\n",
      "epoch 310 : loss 16.414412 ; accuracy 0.8193167\n",
      "epoch 311 : loss 16.391281 ; accuracy 0.8195\n",
      "epoch 312 : loss 16.368254 ; accuracy 0.8197167\n",
      "epoch 313 : loss 16.345331 ; accuracy 0.81995\n",
      "epoch 314 : loss 16.322502 ; accuracy 0.82016665\n",
      "epoch 315 : loss 16.29977 ; accuracy 0.8203167\n",
      "epoch 316 : loss 16.277134 ; accuracy 0.82053334\n",
      "epoch 317 : loss 16.25459 ; accuracy 0.82091665\n",
      "epoch 318 : loss 16.232141 ; accuracy 0.82101667\n",
      "epoch 319 : loss 16.209784 ; accuracy 0.82121664\n",
      "epoch 320 : loss 16.187515 ; accuracy 0.82136667\n",
      "epoch 321 : loss 16.16534 ; accuracy 0.8215333\n",
      "epoch 322 : loss 16.143251 ; accuracy 0.8217\n",
      "epoch 323 : loss 16.12125 ; accuracy 0.8218833\n",
      "epoch 324 : loss 16.099335 ; accuracy 0.82208335\n",
      "epoch 325 : loss 16.077501 ; accuracy 0.8222333\n",
      "epoch 326 : loss 16.055752 ; accuracy 0.82243335\n",
      "epoch 327 : loss 16.03408 ; accuracy 0.8226\n",
      "epoch 328 : loss 16.012493 ; accuracy 0.82275\n",
      "epoch 329 : loss 15.990983 ; accuracy 0.8228667\n",
      "epoch 330 : loss 15.96955 ; accuracy 0.82305\n",
      "epoch 331 : loss 15.948198 ; accuracy 0.82316667\n",
      "epoch 332 : loss 15.926922 ; accuracy 0.8234\n",
      "epoch 333 : loss 15.905725 ; accuracy 0.8236167\n",
      "epoch 334 : loss 15.884606 ; accuracy 0.82376665\n",
      "epoch 335 : loss 15.863566 ; accuracy 0.82393336\n",
      "epoch 336 : loss 15.842605 ; accuracy 0.8241\n",
      "epoch 337 : loss 15.821719 ; accuracy 0.82428336\n",
      "epoch 338 : loss 15.800907 ; accuracy 0.82441664\n",
      "epoch 339 : loss 15.78017 ; accuracy 0.82456666\n",
      "epoch 340 : loss 15.759506 ; accuracy 0.8247\n",
      "epoch 341 : loss 15.7389145 ; accuracy 0.82486665\n",
      "epoch 342 : loss 15.718398 ; accuracy 0.82505\n",
      "epoch 343 : loss 15.697958 ; accuracy 0.8252\n",
      "epoch 344 : loss 15.677594 ; accuracy 0.82535\n",
      "epoch 345 : loss 15.657303 ; accuracy 0.8254667\n",
      "epoch 346 : loss 15.637092 ; accuracy 0.8257\n",
      "epoch 347 : loss 15.616959 ; accuracy 0.8258833\n",
      "epoch 348 : loss 15.596902 ; accuracy 0.8260667\n",
      "epoch 349 : loss 15.576925 ; accuracy 0.82615\n",
      "epoch 350 : loss 15.557027 ; accuracy 0.82626665\n",
      "epoch 351 : loss 15.537215 ; accuracy 0.8264667\n",
      "epoch 352 : loss 15.517485 ; accuracy 0.82666665\n",
      "epoch 353 : loss 15.497835 ; accuracy 0.827\n",
      "epoch 354 : loss 15.478271 ; accuracy 0.8271833\n",
      "epoch 355 : loss 15.458786 ; accuracy 0.82731664\n",
      "epoch 356 : loss 15.439384 ; accuracy 0.82743335\n",
      "epoch 357 : loss 15.420069 ; accuracy 0.8276167\n",
      "epoch 358 : loss 15.400835 ; accuracy 0.82775\n",
      "epoch 359 : loss 15.38169 ; accuracy 0.8279833\n",
      "epoch 360 : loss 15.362623 ; accuracy 0.82816666\n",
      "epoch 361 : loss 15.343637 ; accuracy 0.82825\n",
      "epoch 362 : loss 15.324731 ; accuracy 0.8285\n",
      "epoch 363 : loss 15.305907 ; accuracy 0.8287\n",
      "epoch 364 : loss 15.287161 ; accuracy 0.82883334\n",
      "epoch 365 : loss 15.268489 ; accuracy 0.82918334\n",
      "epoch 366 : loss 15.249894 ; accuracy 0.82935\n",
      "epoch 367 : loss 15.231373 ; accuracy 0.82953334\n",
      "epoch 368 : loss 15.212926 ; accuracy 0.8296667\n",
      "epoch 369 : loss 15.19455 ; accuracy 0.82981664\n",
      "epoch 370 : loss 15.176248 ; accuracy 0.83003336\n",
      "epoch 371 : loss 15.158018 ; accuracy 0.8300833\n",
      "epoch 372 : loss 15.139856 ; accuracy 0.83028334\n",
      "epoch 373 : loss 15.121767 ; accuracy 0.8304\n",
      "epoch 374 : loss 15.10375 ; accuracy 0.83058333\n",
      "epoch 375 : loss 15.085804 ; accuracy 0.8308\n",
      "epoch 376 : loss 15.067929 ; accuracy 0.83093333\n",
      "epoch 377 : loss 15.050123 ; accuracy 0.83106667\n",
      "epoch 378 : loss 15.032383 ; accuracy 0.8311\n",
      "epoch 379 : loss 15.014709 ; accuracy 0.83128333\n",
      "epoch 380 : loss 14.997107 ; accuracy 0.83143336\n",
      "epoch 381 : loss 14.9795685 ; accuracy 0.83166665\n",
      "epoch 382 : loss 14.962099 ; accuracy 0.8319333\n",
      "epoch 383 : loss 14.944698 ; accuracy 0.83205\n",
      "epoch 384 : loss 14.927362 ; accuracy 0.8322333\n",
      "epoch 385 : loss 14.9100895 ; accuracy 0.83231664\n",
      "epoch 386 : loss 14.892879 ; accuracy 0.83243334\n",
      "epoch 387 : loss 14.875727 ; accuracy 0.8325833\n",
      "epoch 388 : loss 14.858635 ; accuracy 0.83276665\n",
      "epoch 389 : loss 14.8416 ; accuracy 0.83295\n",
      "epoch 390 : loss 14.824618 ; accuracy 0.8331\n",
      "epoch 391 : loss 14.80769 ; accuracy 0.8332\n",
      "epoch 392 : loss 14.7908125 ; accuracy 0.83335\n",
      "epoch 393 : loss 14.773984 ; accuracy 0.83346665\n",
      "epoch 394 : loss 14.757206 ; accuracy 0.83353335\n",
      "epoch 395 : loss 14.740477 ; accuracy 0.8336667\n",
      "epoch 396 : loss 14.723796 ; accuracy 0.83376664\n",
      "epoch 397 : loss 14.707164 ; accuracy 0.83383334\n",
      "epoch 398 : loss 14.690579 ; accuracy 0.83395\n",
      "epoch 399 : loss 14.674044 ; accuracy 0.8340333\n",
      "epoch 400 : loss 14.657557 ; accuracy 0.83421665\n",
      "epoch 401 : loss 14.641121 ; accuracy 0.83433336\n",
      "epoch 402 : loss 14.624733 ; accuracy 0.8344167\n",
      "epoch 403 : loss 14.608396 ; accuracy 0.8346\n",
      "epoch 404 : loss 14.592107 ; accuracy 0.8347\n",
      "epoch 405 : loss 14.575871 ; accuracy 0.83485\n",
      "epoch 406 : loss 14.5596895 ; accuracy 0.83496666\n",
      "epoch 407 : loss 14.543558 ; accuracy 0.8350833\n",
      "epoch 408 : loss 14.527479 ; accuracy 0.83521664\n",
      "epoch 409 : loss 14.5114565 ; accuracy 0.83531666\n",
      "epoch 410 : loss 14.495486 ; accuracy 0.8354167\n",
      "epoch 411 : loss 14.47957 ; accuracy 0.83556664\n",
      "epoch 412 : loss 14.463711 ; accuracy 0.83573335\n",
      "epoch 413 : loss 14.447905 ; accuracy 0.83585\n",
      "epoch 414 : loss 14.432157 ; accuracy 0.8359\n",
      "epoch 415 : loss 14.416465 ; accuracy 0.83605\n",
      "epoch 416 : loss 14.40083 ; accuracy 0.83625\n",
      "epoch 417 : loss 14.385252 ; accuracy 0.83633333\n",
      "epoch 418 : loss 14.369727 ; accuracy 0.83646667\n",
      "epoch 419 : loss 14.354259 ; accuracy 0.8366167\n",
      "epoch 420 : loss 14.338842 ; accuracy 0.8367\n",
      "epoch 421 : loss 14.323482 ; accuracy 0.83695\n",
      "epoch 422 : loss 14.308171 ; accuracy 0.83711666\n",
      "epoch 423 : loss 14.2929125 ; accuracy 0.8372167\n",
      "epoch 424 : loss 14.277707 ; accuracy 0.8373167\n",
      "epoch 425 : loss 14.262552 ; accuracy 0.83746666\n",
      "epoch 426 : loss 14.247451 ; accuracy 0.83755\n",
      "epoch 427 : loss 14.232397 ; accuracy 0.8377333\n",
      "epoch 428 : loss 14.217396 ; accuracy 0.8379\n",
      "epoch 429 : loss 14.202441 ; accuracy 0.8379833\n",
      "epoch 430 : loss 14.187542 ; accuracy 0.838\n",
      "epoch 431 : loss 14.172688 ; accuracy 0.83806664\n",
      "epoch 432 : loss 14.157884 ; accuracy 0.83815\n",
      "epoch 433 : loss 14.143128 ; accuracy 0.8383333\n",
      "epoch 434 : loss 14.128421 ; accuracy 0.83848333\n",
      "epoch 435 : loss 14.11376 ; accuracy 0.83863336\n",
      "epoch 436 : loss 14.099147 ; accuracy 0.8388\n",
      "epoch 437 : loss 14.084583 ; accuracy 0.83895\n",
      "epoch 438 : loss 14.070066 ; accuracy 0.8390833\n",
      "epoch 439 : loss 14.0555935 ; accuracy 0.83926666\n",
      "epoch 440 : loss 14.041173 ; accuracy 0.8394\n",
      "epoch 441 : loss 14.0267935 ; accuracy 0.8395\n",
      "epoch 442 : loss 14.012463 ; accuracy 0.83973336\n",
      "epoch 443 : loss 13.998175 ; accuracy 0.83986664\n",
      "epoch 444 : loss 13.983932 ; accuracy 0.83998334\n",
      "epoch 445 : loss 13.969733 ; accuracy 0.84015\n",
      "epoch 446 : loss 13.955578 ; accuracy 0.84025\n",
      "epoch 447 : loss 13.941466 ; accuracy 0.84036666\n",
      "epoch 448 : loss 13.927401 ; accuracy 0.8404833\n",
      "epoch 449 : loss 13.913379 ; accuracy 0.8405833\n",
      "epoch 450 : loss 13.8994 ; accuracy 0.84066665\n",
      "epoch 451 : loss 13.885465 ; accuracy 0.84075\n",
      "epoch 452 : loss 13.871571 ; accuracy 0.8408167\n",
      "epoch 453 : loss 13.8577175 ; accuracy 0.841\n",
      "epoch 454 : loss 13.8439045 ; accuracy 0.84106666\n",
      "epoch 455 : loss 13.830135 ; accuracy 0.84111667\n",
      "epoch 456 : loss 13.816402 ; accuracy 0.84125\n",
      "epoch 457 : loss 13.802709 ; accuracy 0.84135\n",
      "epoch 458 : loss 13.789055 ; accuracy 0.84136665\n",
      "epoch 459 : loss 13.775442 ; accuracy 0.8415\n",
      "epoch 460 : loss 13.761867 ; accuracy 0.8415667\n",
      "epoch 461 : loss 13.748333 ; accuracy 0.84166664\n",
      "epoch 462 : loss 13.734837 ; accuracy 0.84175\n",
      "epoch 463 : loss 13.721379 ; accuracy 0.8419\n",
      "epoch 464 : loss 13.707958 ; accuracy 0.8419333\n",
      "epoch 465 : loss 13.694575 ; accuracy 0.84205\n",
      "epoch 466 : loss 13.681231 ; accuracy 0.84216666\n",
      "epoch 467 : loss 13.667924 ; accuracy 0.8423\n",
      "epoch 468 : loss 13.654655 ; accuracy 0.8423667\n",
      "epoch 469 : loss 13.641423 ; accuracy 0.84243333\n",
      "epoch 470 : loss 13.628226 ; accuracy 0.8426\n",
      "epoch 471 : loss 13.615067 ; accuracy 0.84276664\n",
      "epoch 472 : loss 13.60194 ; accuracy 0.84286666\n",
      "epoch 473 : loss 13.588852 ; accuracy 0.84291667\n",
      "epoch 474 : loss 13.575797 ; accuracy 0.8429667\n",
      "epoch 475 : loss 13.5627775 ; accuracy 0.8430833\n",
      "epoch 476 : loss 13.549788 ; accuracy 0.84315\n",
      "epoch 477 : loss 13.536835 ; accuracy 0.84326667\n",
      "epoch 478 : loss 13.523915 ; accuracy 0.8434\n",
      "epoch 479 : loss 13.51103 ; accuracy 0.84353334\n",
      "epoch 480 : loss 13.498181 ; accuracy 0.84368336\n",
      "epoch 481 : loss 13.485365 ; accuracy 0.8437667\n",
      "epoch 482 : loss 13.472581 ; accuracy 0.84385\n",
      "epoch 483 : loss 13.459829 ; accuracy 0.84395\n",
      "epoch 484 : loss 13.447111 ; accuracy 0.8441667\n",
      "epoch 485 : loss 13.434428 ; accuracy 0.84421664\n",
      "epoch 486 : loss 13.421778 ; accuracy 0.84428334\n",
      "epoch 487 : loss 13.409166 ; accuracy 0.84435\n",
      "epoch 488 : loss 13.396587 ; accuracy 0.8445167\n",
      "epoch 489 : loss 13.384042 ; accuracy 0.84456664\n",
      "epoch 490 : loss 13.3715315 ; accuracy 0.84466666\n",
      "epoch 491 : loss 13.359058 ; accuracy 0.8447667\n",
      "epoch 492 : loss 13.3466215 ; accuracy 0.8449\n",
      "epoch 493 : loss 13.334221 ; accuracy 0.84503335\n",
      "epoch 494 : loss 13.3218565 ; accuracy 0.8452333\n",
      "epoch 495 : loss 13.309529 ; accuracy 0.84533334\n",
      "epoch 496 : loss 13.2972355 ; accuracy 0.84548336\n",
      "epoch 497 : loss 13.284979 ; accuracy 0.8455833\n",
      "epoch 498 : loss 13.2727585 ; accuracy 0.84568334\n",
      "epoch 499 : loss 13.2605715 ; accuracy 0.84571666\n",
      "test loss 13.269062 ; accuracy 0.8483\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "train_data, test_data = mnist_dataset()\n",
    "for epoch in range(500):\n",
    "    loss, accuracy = train_one_step(model, optimizer, \n",
    "                                    tf.constant(train_data[0], dtype=tf.float32), \n",
    "                                    tf.constant(train_data[1], dtype=tf.int64))\n",
    "    print('epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())\n",
    "loss, accuracy = test(model, \n",
    "                      tf.constant(test_data[0], dtype=tf.float32), \n",
    "                      tf.constant(test_data[1], dtype=tf.int64))\n",
    "\n",
    "print('test loss', loss.numpy(), '; accuracy', accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}